{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/models') \n",
    "sys.path.append('../src/data') \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading\t train_dataset\n",
      "loaded:\t train_dataset\n",
      "loading\t cv_dataset\n",
      "loaded:\t cv_dataset\n",
      "loading\t test_dataset\n",
      "loaded:\t test_dataset\n",
      "train_dataset: 110 examples\n",
      "cv_dataset: 12140 examples\n",
      "test_dataset: 12747 examples\n"
     ]
    }
   ],
   "source": [
    "from model1.dataloader import load_data\n",
    "train_dataset, cv_dataset, test_dataset, feature_info = load_data(sample_train= 0.001, feature_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(train_dataset))\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_dataset = cv_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'group_feature_size': 463, 'technique_feature_size': 54}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# build model from config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('config.yaml', 'r') as config_file:\n",
    "    config = yaml.safe_load (config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture_config = config['model_architecture']\n",
    "train_config = config['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'group_nn_widths': [3, 4, 5, 6],\n",
       " 'group_nn_depth': 6,\n",
       " 'technique_nn_widths': [3, 4, 5, 6],\n",
       " 'technique_nn_depth': 6,\n",
       " 'nn_output_size': 4}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_architecture_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customNN(keras.Sequential):\n",
    "    def __init__(self, \n",
    "                 name,\n",
    "                 input_size, \n",
    "                 output_size,\n",
    "                 widths: int|list,\n",
    "                 depth: int,\n",
    "                 hidden_layer_activation = 'relu',\n",
    "                 output_layer_activation = 'linear',\n",
    "                 ):\n",
    "        super().__init__(name = name)\n",
    "        \"\"\"\n",
    "        when there is a single int for `widths`, all the Dense layers are identical in size\n",
    "        `widths` only indicates the widths in the middle layer, NOT the last layer. \n",
    "        The last layer's widths is indicated by `output_size`\n",
    "        \"\"\"\n",
    "        if isinstance (widths, int):\n",
    "            # First dense layer defined with input shape\n",
    "            self.add(keras.layers.Dense(widths, input_shape=(input_size,)))\n",
    "            # Custom layer of hidden Dense layer\n",
    "            for _ in range (depth-2):\n",
    "                self.add (keras.layers.Dense(widths,activation= hidden_layer_activation))\n",
    "            # Output layer\n",
    "            self.add (keras.layers.Dense (output_size, activation = output_layer_activation, name = 'output_NN'))  \n",
    "        elif isinstance (widths, list) and len(widths) == (depth-2):\n",
    "            self.add(keras.layers.Dense(widths[0], input_shape=(input_size,)))\n",
    "            for width in widths[1:]:\n",
    "                self.add (keras.layers.Dense(width,activation= hidden_layer_activation))\n",
    "            self.add (keras.layers.Dense (output_size, activation = output_layer_activation, name = 'output_NN'))  \n",
    "        else: raise Exception (\"CustomNN: widths and depths are set incorrectly.\\n Most likely becase: widths is a list, and len(width) == (depth-2) is False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customModel(keras.Model):\n",
    "    def __init__(self, input_sizes, name = None,\n",
    "                 group_nn_widths = None, group_nn_depth = None, \n",
    "                 technique_nn_widths = None, technique_nn_depth = None,\n",
    "                 nn_output_size = None, config = None,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(name = name, *args, **kwargs)\n",
    "        \n",
    "        if config != None:\n",
    "            print ('---build from config')\n",
    "            group_nn_widths = config['group_nn_widths']\n",
    "            group_nn_depth = config['group_nn_depth']\n",
    "            technique_nn_widths = config['technique_nn_widths']\n",
    "            technique_nn_depth = config['technique_nn_depth']\n",
    "            nn_output_size = config['nn_output_size']\n",
    "            \n",
    "        group_input_size = input_sizes['group_feature_size']\n",
    "        technique_input_size = input_sizes['technique_feature_size']\n",
    "        \n",
    "        self.input_Group = keras.layers.Input (shape= (group_input_size,), name = 'input_Group')\n",
    "        self.input_Technique = keras.layers.Input (shape= (technique_input_size,), name = 'input_Technique')\n",
    "        self.Group_NN = customNN(input_size =  group_input_size,\n",
    "                                 output_size = nn_output_size,\n",
    "                                 widths = group_nn_widths,\n",
    "                                 depth =group_nn_depth,\n",
    "                                 name = 'Group_NN')\n",
    "        self.Technique_NN = customNN(input_size = technique_input_size,\n",
    "                                 output_size = nn_output_size,\n",
    "                                 widths = technique_nn_widths,\n",
    "                                 depth = technique_nn_depth,\n",
    "                                 name = 'Technique_NN')\n",
    "        \n",
    "        self.dot_product = keras.layers.Dot(axes= 1)\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # self.input_NN_1, self.input_NN_2 = inputs[0], inputs[1]\n",
    "        self.input_Group = inputs['input_Group']\n",
    "        self.input_Technique = inputs['input_Technique']\n",
    "        \n",
    "        output_Group = self.Group_NN(self.input_Group)\n",
    "        output_Technique = self.Technique_NN(self.input_Technique)\n",
    "        \n",
    "        norm_output_Group = tf.linalg.l2_normalize (output_Group, axis = 1)\n",
    "        norm_output_Technique = tf.linalg.l2_normalize (output_Technique, axis = 1)\n",
    "        \n",
    "        dot_product = self.dot_product ([norm_output_Group, norm_output_Technique])\n",
    "        return dot_product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---config FOUND\n"
     ]
    }
   ],
   "source": [
    "model = customModel (\n",
    "    input_sizes= feature_info,\n",
    "    config= model_architecture_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam (learning_rate= 0.001)    \n",
    "loss = keras.losses.BinaryCrossentropy (from_logits= True)\n",
    "model.compile (optimizer, loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 7s 256ms/step - loss: 0.7267 - val_loss: 0.6944\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    validation_data= cv_dataset,\n",
    "    epochs= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Group_NN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 3)                 1392      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 16        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 25        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 36        \n",
      "                                                                 \n",
      " output_NN (Dense)           (None, 4)                 28        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,497\n",
      "Trainable params: 1,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.Group_NN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Train model from config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading\t train_dataset\n",
      "loaded:\t train_dataset\n",
      "loading\t cv_dataset\n",
      "loaded:\t cv_dataset\n",
      "loading\t test_dataset\n",
      "loaded:\t test_dataset\n",
      "train_dataset: 110 examples\n",
      "cv_dataset: 12140 examples\n",
      "test_dataset: 12747 examples\n"
     ]
    }
   ],
   "source": [
    "from model1.dataloader import load_data\n",
    "train_dataset, cv_dataset, test_dataset, feature_info = load_data(sample_train= 0.001, feature_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = config['train']\n",
    "batch_size = train_config['batch_size']\n",
    "epochs = train_config['epochs']\n",
    "learning_rate = train_config['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 20, 'learning_rate': 0.001, 'batch_size': 32}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(train_dataset))\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_dataset = cv_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---config FOUND\n"
     ]
    }
   ],
   "source": [
    "model = customModel (\n",
    "    input_sizes= feature_info,\n",
    "    config= model_architecture_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam (learning_rate= learning_rate)    \n",
    "loss = keras.losses.BinaryCrossentropy (from_logits= True)\n",
    "model.compile (optimizer, loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 8s 239ms/step - loss: 0.7016 - val_loss: 0.5083\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 5s 191ms/step - loss: 0.7539 - val_loss: 0.4948\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 5s 189ms/step - loss: 0.7031 - val_loss: 0.5004\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 5s 191ms/step - loss: 0.6540 - val_loss: 0.5165\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 6s 194ms/step - loss: 0.7230 - val_loss: 0.5321\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 5s 188ms/step - loss: 0.6997 - val_loss: 0.5362\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 6s 190ms/step - loss: 0.7013 - val_loss: 0.5359\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 5s 213ms/step - loss: 0.7005 - val_loss: 0.5390\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 5s 190ms/step - loss: 0.7387 - val_loss: 0.5526\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 5s 189ms/step - loss: 0.6683 - val_loss: 0.5641\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 5s 191ms/step - loss: 0.7029 - val_loss: 0.5595\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 5s 195ms/step - loss: 0.6487 - val_loss: 0.5507\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 5s 189ms/step - loss: 0.7167 - val_loss: 0.5615\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 5s 186ms/step - loss: 0.7089 - val_loss: 0.5845\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 5s 226ms/step - loss: 0.6901 - val_loss: 0.6124\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 5s 189ms/step - loss: 0.6714 - val_loss: 0.6279\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 5s 186ms/step - loss: 0.7361 - val_loss: 0.6536\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 5s 189ms/step - loss: 0.6485 - val_loss: 0.6390\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 5s 188ms/step - loss: 0.6975 - val_loss: 0.6173\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 5s 199ms/step - loss: 0.6587 - val_loss: 0.5986\n"
     ]
    }
   ],
   "source": [
    "history = model.fit (\n",
    "    train_dataset,\n",
    "    validation_data= cv_dataset,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv('training_history.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcv-gt24-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
