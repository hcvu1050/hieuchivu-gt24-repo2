{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/models') \n",
    "sys.path.append('../src/data') \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model1.dataloader import load_data\n",
    "from model1.model_v0_3 import ContentBasedFiltering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading\t train_dataset\n",
      "loaded:\t train_dataset\n",
      "loading\t cv_dataset\n",
      "loaded:\t cv_dataset\n",
      "loading\t test_dataset\n",
      "loaded:\t test_dataset\n",
      "train_dataset: 5549 examples\n",
      "cv_dataset: 12140 examples\n",
      "test_dataset: 12747 examples\n"
     ]
    }
   ],
   "source": [
    "train_dataset, cv_dataset, test_dataset, info = load_data(sample_train= 0.05, feature_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(train_dataset))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_dataset = cv_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_G_features = info['X_group_num_features']\n",
    "num_T_features = info['X_technique_num_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ContentBasedFiltering (\n",
    "    num_G_features=num_G_features, \n",
    "    num_T_features=num_T_features,\n",
    "    Group_NN_width = 16,\n",
    "    Group_NN_depth = 3,\n",
    "    Technique_NN_width = 16,\n",
    "    Technique_NN_depth = 3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"recsysNN_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_Group (InputLayer)       [(None, 463)]        0           []                               \n",
      "                                                                                                  \n",
      " input_Technique (InputLayer)   [(None, 54)]         0           []                               \n",
      "                                                                                                  \n",
      " Group_NN (Sequential)          (None, 32)           8512        ['input_Group[0][0]']            \n",
      "                                                                                                  \n",
      " Technique_NN (Sequential)      (None, 32)           1968        ['input_Technique[0][0]']        \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1)            0           ['Group_NN[0][0]',               \n",
      "                                                                  'Technique_NN[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,480\n",
      "Trainable params: 10,480\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "174/174 [==============================] - 8s 6ms/step - loss: 0.5778 - val_loss: 0.6213\n",
      "Epoch 2/5\n",
      "174/174 [==============================] - 6s 6ms/step - loss: 0.5221 - val_loss: 0.5743\n",
      "Epoch 3/5\n",
      "174/174 [==============================] - 7s 6ms/step - loss: 0.4971 - val_loss: 0.6330\n",
      "Epoch 4/5\n",
      "174/174 [==============================] - 6s 6ms/step - loss: 0.5027 - val_loss: 0.5149\n",
      "Epoch 5/5\n",
      "174/174 [==============================] - 6s 6ms/step - loss: 0.4938 - val_loss: 0.5153\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "history = model1.train (\n",
    "    train_dataset,\n",
    "    val_data = cv_dataset,\n",
    "    epochs= epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable float object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m test_dataset \u001b[39m=\u001b[39m test_dataset\u001b[39m.\u001b[39mbatch(batch_size)\n\u001b[1;32m----> 3\u001b[0m loss, accuracy \u001b[39m=\u001b[39m model1\u001b[39m.\u001b[39mevaluate (test_dataset)\n",
      "File \u001b[1;32mc:\\Users\\vuchi\\GT24\\hieuchivu-gt24-repo2\\notebooks\\../src/models\\model1\\model_v0_3.py:96\u001b[0m, in \u001b[0;36mContentBasedFiltering.evaluate\u001b[1;34m(self, test_dataset)\u001b[0m\n\u001b[0;32m     92\u001b[0m total_samples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     94\u001b[0m \u001b[39mfor\u001b[39;00m x_batch, y_batch \u001b[39min\u001b[39;00m test_dataset:\n\u001b[0;32m     95\u001b[0m     \u001b[39m# Compute predictions and loss for each batch\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m     batch_loss, batch_accuracy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mevaluate(x_batch, y_batch, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     98\u001b[0m     \u001b[39m# Update total loss and accuracy\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss \u001b[39m*\u001b[39m x_batch\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable float object"
     ]
    }
   ],
   "source": [
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "loss, accuracy = model1.evaluate (test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcv-gt24-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
